{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import time \n",
    "import torch.nn as  nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(torch.tensor([1,2,3,4 ,99,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2119e+25,  4.9658e+28,  2.6890e+32,  6.4577e+16,  3.4736e-12,\n",
       "          3.9604e-11],\n",
       "        [ 7.5640e+23,  1.5793e-19,  5.7704e-05,  2.5353e+30,  3.3351e-43,\n",
       "          4.1773e-41],\n",
       "        [ 6.5612e-39,  1.5488e+04,  2.5353e+30,  2.7465e-43,  2.1684e-41,\n",
       "          2.6177e+30],\n",
       "        [ 3.9376e-43,  1.0060e-40,  1.2255e-38,  8.0071e-22,  1.0141e+31,\n",
       "          5.6052e-45],\n",
       "        [ 3.5059e+00,  1.6661e-41,  1.0221e+19,  1.1625e+27,  1.6529e+19,\n",
       "          1.8337e+31],\n",
       "        [ 1.0312e-40,  2.6582e-40,  4.3429e-38, -4.0240e+34,  1.1844e-38,\n",
       "         -2.8602e-29]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your\n",
    "[0.55, 0.87, 0.66], # journey\n",
    "[0.57, 0.85, 0.64], # starts\n",
    "[0.22, 0.58, 0.33], # with\n",
    "[0.77, 0.25, 0.10], # one\n",
    "[0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Time taken for the code block: 0.005800 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time  = time.time()\n",
    "attetion_score = torch.empty(6 ,6)\n",
    "for i , x_i in enumerate(inputs):\n",
    "    for j , x_j in enumerate(inputs):\n",
    "        attetion_score[i,j] = torch.dot(x_i ,x_j)\n",
    "\n",
    "print(attetion_score)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken for the code block: {end_time - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Time taken for the code block: 0.005135 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time  = time.time()\n",
    "ttetion_score = inputs @ inputs.T \n",
    "end_time = time.time()\n",
    "print(attetion_score)\n",
    "print(f\"Time taken for the code block: {end_time - start_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_V1(nn.Module):\n",
    "        def __init__(self,d_in , d_out):\n",
    "                super().__init__()\n",
    "                self.d_out = d_out\n",
    "                self.w_query = nn.Parameter(torch.randn(d_in , d_out))\n",
    "                self.w_key = nn.Parameter(torch.randn(d_in , d_out))\n",
    "                self.w_value = nn.Parameter(torch.randn(d_in , d_out))\n",
    "        def forward(self, x):\n",
    "                keys = x @ self.w_key\n",
    "                queries = x @ self.w_query\n",
    "                values = x @ self.w_value\n",
    "                atten_score = queries @ keys.T\n",
    "                atten_weight  = torch.softmax(atten_score/keys.shape[-1]** 0.5 , dim =1)\n",
    "                context_vector = atten_weight @ values\n",
    "                return context_vector\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2845, 0.4071],\n",
      "        [0.2854, 0.4081],\n",
      "        [0.2854, 0.4075],\n",
      "        [0.2864, 0.3974],\n",
      "        [0.2863, 0.3910],\n",
      "        [0.2860, 0.4039]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sel_att  =SelfAttention_V1(3 , 2)\n",
    "print(sel_att(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attention class using the Linear Layer \n",
    "class SelfAttention_v2(nn.Module):\n",
    "                def __init__(self, d_in, d_out, qkv_bias  = False):\n",
    "                        super().__init__()\n",
    "                        self.d_out = d_out \n",
    "                        self.w_query = nn.Linear(d_in, d_out , bias=qkv_bias)\n",
    "                        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n",
    "                        self.w_value = nn.Linear(d_in , d_out, bias=qkv_bias)\n",
    "                        \n",
    "                def forward(self, x):\n",
    "                        keys = self.w_key(x)\n",
    "                        queries = self.w_query(x)\n",
    "                        values  =  self.w_value(x)\n",
    "                        attetion_score = queries @ keys.T\n",
    "                        atten_weight = torch.softmax(attetion_score / keys.shape[-1]**0.5 , dim =1)\n",
    "                        contxt_vec = atten_weight @ values\n",
    "                        return contxt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "d_in  =3 \n",
    "d_out =2\n",
    "sa_v2  = SelfAttention_v2(d_in , d_out)\n",
    "print(sa_v2(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
      "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
      "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
      "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
      "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
      "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[0.1677, 0.1666, 0.1666, 0.1662, 0.1669, 0.1660],\n",
      "        [0.1682, 0.1667, 0.1667, 0.1659, 0.1667, 0.1658],\n",
      "        [0.1682, 0.1667, 0.1667, 0.1659, 0.1667, 0.1658],\n",
      "        [0.1675, 0.1667, 0.1667, 0.1662, 0.1667, 0.1662],\n",
      "        [0.1674, 0.1667, 0.1667, 0.1663, 0.1666, 0.1663],\n",
      "        [0.1678, 0.1667, 0.1667, 0.1661, 0.1667, 0.1661]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "query = sa_v2.w_query(inputs)\n",
    "keys = sa_v2.w_key(inputs)\n",
    "atten_score = query @ keys.T \n",
    "print(atten_score)\n",
    "atten_weight  = torch.softmax(atten_score  / keys.shape[-1]**5 , dim= 1)\n",
    "print(atten_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24,  0,  0,  0],\n",
       "        [13,  4,  0,  0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.tensor([[24,35,46,6] ,[13,4,56,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = atten_weight.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length , context_length))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1677, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1682, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1682, 0.1667, 0.1667, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1675, 0.1667, 0.1667, 0.1662, 0.0000, 0.0000],\n",
       "        [0.1674, 0.1667, 0.1667, 0.1663, 0.1666, 0.0000],\n",
       "        [0.1678, 0.1667, 0.1667, 0.1661, 0.1667, 0.1661]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sample = atten_weight * mask_simple\n",
    "masked_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1677, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1682, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1682, 0.1667, 0.1667, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1675, 0.1667, 0.1667, 0.1662, 0.0000, 0.0000],\n",
       "        [0.1674, 0.1667, 0.1667, 0.1663, 0.1666, 0.0000],\n",
       "        [0.1678, 0.1667, 0.1667, 0.1661, 0.1667, 0.1661]],\n",
       "       grad_fn=<TrilBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(atten_weight , diagonal=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1677],\n",
       "        [0.3349],\n",
       "        [0.5015],\n",
       "        [0.6671],\n",
       "        [0.8337],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sample.sum(dim=1 , keepdim= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sum  = masked_sample.sum(dim=1 , keepdim= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5023, 0.4977, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3353, 0.3323, 0.3323, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2511, 0.2498, 0.2499, 0.2492, 0.0000, 0.0000],\n",
       "        [0.2008, 0.1999, 0.1999, 0.1995, 0.1999, 0.0000],\n",
       "        [0.1678, 0.1667, 0.1667, 0.1661, 0.1667, 0.1661]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sample_norm = masked_sample / row_sum\n",
    "masked_sample_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 23,  4,  5],\n",
       "        [ 0,  0, 45,  6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.tensor([[2,23,4,5 ] ,[2,33,45,6]]) , diagonal= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "masked = atten_score.masked_fill(mask.bool(), -torch.inf)\n",
    "\n",
    "\n",
    "\n",
    "masked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000],\n",
       "        [0.4656, 0.1723, 2.0000, 2.0000, 2.0000, 2.0000],\n",
       "        [0.4594, 0.1703, 0.1731, 2.0000, 2.0000, 2.0000],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186, 2.0000, 2.0000],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786, 2.0000],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_score.masked_fill(mask.bool() ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = atten_score.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(context_length , context_length) , diagonal=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(context_length , context_length) , diagonal=1 ).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask  = torch.triu(torch.ones(context_length , context_length) , diagonal=1 )\n",
    "masked =atten_score.masked_fill(mask.bool() ,-torch.inf )\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5023, 0.4977, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3353, 0.3323, 0.3323, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2511, 0.2498, 0.2499, 0.2492, 0.0000, 0.0000],\n",
       "        [0.2008, 0.1999, 0.1999, 0.1995, 0.1999, 0.0000],\n",
       "        [0.1678, 0.1667, 0.1667, 0.1661, 0.1667, 0.1661]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_weights = torch.softmax(masked / keys.shape[-1]**5 ,dim = 1)\n",
    "atten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout  = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(example)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9954, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6647, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5022, 0.4997, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4015, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(torch.nn.Dropout(0.5)(atten_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs , inputs), dim=0 )\n",
    "print(batch)\n",
    "print(batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
       "        [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
       "        [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in , d_out , context_length , dropout ,    qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.w_query = nn.Linear(d_in , d_out, bias= qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in , d_out, bias =qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in , d_out , bias  = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "        diagonal=1))\n",
    "    def forward(self, x):\n",
    "        b , num_tokens , d_in = x.shape\n",
    "        keys  = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values  = self.w_value(x)\n",
    "        atten_scores = queries @ keys.transpose(1, 2)\n",
    "        atten_scores.masked_fill_( #D\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)        \n",
    "        # print(atten_scores)\n",
    "        atten_weights = torch.softmax(atten_scores  / keys.shape[-1]**0.5 , dim= 1)\n",
    "        # print(atten_weights)\n",
    "        atten_weights = self.dropout(atten_weights)\n",
    "        context_vector = atten_weights @ values\n",
    "        return context_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key= nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) #A\n",
    "        self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "        diagonal=1)\n",
    "        ) #B\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2) #C\n",
    "        attn_scores.masked_fill_( #D\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: tensor([[[-0.0844,  0.0414],\n",
      "         [-0.2264, -0.0039],\n",
      "         [-0.4163, -0.0564],\n",
      "         [-0.5014, -0.1011],\n",
      "         [-0.7754, -0.1867],\n",
      "         [-1.1632, -0.3303]],\n",
      "\n",
      "        [[-0.0844,  0.0414],\n",
      "         [-0.2264, -0.0039],\n",
      "         [-0.4163, -0.0564],\n",
      "         [-0.5014, -0.1011],\n",
      "         [-0.7754, -0.1867],\n",
      "         [-1.1632, -0.3303]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self ,d_in, d_out , context_length , dropout, num_heads , qkv_bias  = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in=d_in , d_out=d_out , dropout=dropout,context_length=context_length,qkv_bias=qkv_bias)\n",
    "              for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.heads],dim =-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0844,  0.0414,  0.0766,  0.0171],\n",
      "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
      "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
      "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
      "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
      "         [-1.1632, -0.3303,  1.1224,  0.8460]],\n",
      "\n",
      "        [[-0.0844,  0.0414,  0.0766,  0.0171],\n",
      "         [-0.2264, -0.0039,  0.2143,  0.1185],\n",
      "         [-0.4163, -0.0564,  0.3878,  0.2453],\n",
      "         [-0.5014, -0.1011,  0.4992,  0.3401],\n",
      "         [-0.7754, -0.1867,  0.7387,  0.4868],\n",
      "         [-1.1632, -0.3303,  1.1224,  0.8460]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-9.1476e-02,  3.4164e-02],\n",
      "         [-2.6796e-01, -1.3427e-03],\n",
      "         [-4.8421e-01, -4.8909e-02],\n",
      "         [-6.4808e-01, -1.0625e-01],\n",
      "         [-8.8380e-01, -1.7140e-01],\n",
      "         [-1.4744e+00, -3.4327e-01]],\n",
      "\n",
      "        [[-9.1476e-02,  3.4164e-02],\n",
      "         [-2.6796e-01, -1.3427e-03],\n",
      "         [-4.8421e-01, -4.8909e-02],\n",
      "         [-6.4808e-01, -1.0625e-01],\n",
      "         [-8.8380e-01, -1.7140e-01],\n",
      "         [-1.4744e+00, -3.4327e-01]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Exercise :Change the embedding vector dimention to 2 instand of 4\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 1#Change the out dimention into 1 \n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0722,  0.7488,  0.0954,  0.7472,  0.0808,  0.7351],\n",
       "        [-0.0481,  0.4820, -0.2781,  0.3609,  0.1056,  0.6211]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = nn.Linear(3,2)\n",
    "ln(inputs).shape\n",
    "ln(inputs).view(2, 6  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3).view(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2%2 ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 //2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0722,  0.0808, -0.2781],\n",
       "        [ 0.7488,  0.7351,  0.3609],\n",
       "        [ 0.0954, -0.0481,  0.1056],\n",
       "        [ 0.7472,  0.4820,  0.6211]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln(inputs).view(3,4).transpose(-2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_V2(nn.Module):\n",
    "    def __init__(self, d_in , d_out , context_length  , dropout ,num_heads,qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads  == 0,'d_out must be divisible by the num_heads'\n",
    "        self.w_query = nn.Linear(d_in , d_out ,bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in , d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in  , d_out,bias=qkv_bias)\n",
    "        self.d_in =d_in\n",
    "        self.d_out = d_out\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_heads  = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.out_proj  = nn.Linear(d_out , d_out)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length , context_length),diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        b, num_tokens , d_in = x.shape\n",
    "        keys = self.w_key(x)\n",
    "        queries  = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        queries = queries.view(b, num_tokens , self.num_heads , self.head_dim)\n",
    "        values = values.view(b , num_tokens , self.num_heads , self.head_dim)\n",
    "        keys = keys.view( b, num_tokens , self.num_heads , self.head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool= self.mask.bool()[:num_tokens , :num_tokens]\n",
    "        attn_scores.masked_fill(mask_bool , -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores /self.head_dim**0.5   , dim=-1 )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vector = (attn_weights  @ values).transpose(1, 2)\n",
    "        context_vector = context_vector.contiguous().view(b , num_tokens , self.d_out)\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention_V2(\n",
    "        d_in=cfg[\"emb_dim\"],\n",
    "        d_out=cfg[\"emb_dim\"],\n",
    "        context_length=cfg[\"context_length\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout=cfg[\"drop_rate\"],\n",
    "        qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "    #A\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "        shortcut = x #B\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut #C\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
       "          [0.8993, 0.0390, 0.9268, 0.7388],\n",
       "          [0.7179, 0.7058, 0.9156, 0.4340]],\n",
       "\n",
       "         [[0.0772, 0.3565, 0.1479, 0.5331],\n",
       "          [0.4066, 0.2318, 0.4545, 0.9737],\n",
       "          [0.4606, 0.5159, 0.4220, 0.5786]]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(2,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
       "          [0.8993, 0.0390, 0.9268, 0.7388],\n",
       "          [0.7179, 0.7058, 0.9156, 0.4340]],\n",
       "\n",
       "         [[0.0772, 0.3565, 0.1479, 0.5331],\n",
       "          [0.4066, 0.2318, 0.4545, 0.9737],\n",
       "          [0.4606, 0.5159, 0.4220, 0.5786]]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2745, 0.8993, 0.7179],\n",
       "          [0.6584, 0.0390, 0.7058],\n",
       "          [0.2775, 0.9268, 0.9156],\n",
       "          [0.8573, 0.7388, 0.4340]],\n",
       "\n",
       "         [[0.0772, 0.4066, 0.4606],\n",
       "          [0.3565, 0.2318, 0.5159],\n",
       "          [0.1479, 0.4545, 0.4220],\n",
       "          [0.5331, 0.9737, 0.5786]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4, 3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(2,3 ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.3208, 1.1631, 1.2879],\n",
       "          [1.1631, 2.2150, 1.8424],\n",
       "          [1.2879, 1.8424, 2.0402]],\n",
       "\n",
       "         [[0.4391, 0.7003, 0.5903],\n",
       "          [0.7003, 1.3737, 1.0620],\n",
       "          [0.5903, 1.0620, 0.9912]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ a.transpose(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2595, 0.4014],\n",
      "         [0.2583, 0.4014],\n",
      "         [0.2583, 0.4014],\n",
      "         [0.2575, 0.4031],\n",
      "         [0.2582, 0.4026],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.2595, 0.4014],\n",
      "         [0.2583, 0.4014],\n",
      "         [0.2583, 0.4014],\n",
      "         [0.2575, 0.4031],\n",
      "         [0.2582, 0.4026],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention_V2(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8061,  1.8960, -0.1750]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_emb = nn.Embedding(21,3)\n",
    "position_emb(torch.arange(1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9948,  1.2176, -0.2282]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_emb(torch.tensor([20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DummyGPTModel \n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_size'] , cfg['emb_dim'])\n",
    "        self.position_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg)  for _ in range(cfg['n_layers'])])\n",
    "        \n",
    "        self.final_norm = DummyLayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'] , cfg['vocab_size'] , bias= False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size  , seq_len = in_idx.shape\n",
    "        tok_emb = self.token_emb(in_idx)\n",
    "        pos_emb = self.position_emb(torch.arange(seq_len))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x \n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalize_shape  , eps =1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 1024,\n",
    "# Context length\n",
    "\"emb_dim\": 768,\n",
    "# Embedding dimension\n",
    "\"n_heads\": 12,\n",
    "# Number of attention heads\n",
    "\"n_layers\": 12,\n",
    "# Number of layers\n",
    "\"drop_rate\": 0.1,\n",
    "# Dropout rate\n",
    "\"qkv_bias\": False\n",
    "# Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.0098, -1.3371,  1.5360,  ...,  0.9138, -0.0446,  0.5080],\n",
      "         [-0.1986, -2.0926, -1.2976,  ...,  0.4652, -0.6466,  1.8320],\n",
      "         [ 1.6675, -1.6455, -1.4320,  ..., -0.6257,  1.3321,  0.0252],\n",
      "         [ 0.7580,  1.3023, -0.3567,  ..., -0.4735, -0.6564,  0.3811]],\n",
      "\n",
      "        [[-0.9175, -0.5156,  2.0081,  ...,  1.0170,  0.2842,  0.7774],\n",
      "         [-0.1943, -0.2929, -1.4559,  ...,  1.0894, -0.9605,  1.9351],\n",
      "         [ 1.4038, -1.0772, -0.7692,  ..., -0.5776, -0.6314,  0.2204],\n",
      "         [ 0.8002,  1.0435,  0.4070,  ..., -0.1311, -0.5104,  0.7819]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2,5)\n",
    "print(batch_example)\n",
    "layer = nn.Sequential(nn.Linear(5,6) , nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1 , keepdim= True) # dim =0  Column , dim = 1 Row \n",
    "var = out.var(dim=-1 , keepdim= True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1299], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][1] - mean[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0935, grad_fn=<SubBackward0>)\n",
      "tensor(0.2145, grad_fn=<SubBackward0>)\n",
      "tensor(-0.1324, grad_fn=<SubBackward0>)\n",
      "tensor(0.0892, grad_fn=<SubBackward0>)\n",
      "tensor(-0.1324, grad_fn=<SubBackward0>)\n",
      "tensor(-0.1324, grad_fn=<SubBackward0>)\n",
      "tensor(-0.0038, grad_fn=<SubBackward0>)\n",
      "tensor(0.0224, grad_fn=<SubBackward0>)\n",
      "tensor(-0.2170, grad_fn=<SubBackward0>)\n",
      "tensor(0.3028, grad_fn=<SubBackward0>)\n",
      "tensor(0.1127, grad_fn=<SubBackward0>)\n",
      "tensor(-0.2170, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 2):\n",
    "    for j in range(0, 6):\n",
    "    \n",
    "        print(out[i][j]-mean[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6459, -0.5249, -0.8719, -0.6503, -0.8719, -0.8719],\n",
       "        [-0.8744, -0.8482, -1.0876, -0.5678, -0.7579, -1.0876]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out - mean / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim= -1 , keepdim= True)\n",
    "var = out_norm.var(dim =-1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Turn Off the scientific notation \n",
    "torch.set_printoptions(sci_mode= False)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self , emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale  = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim= -1, keepdim = True)\n",
    "        var = x.var(dim =-1, keepdim = True)\n",
    "        norm_x = (x - mean) / torch.sqrt(var +self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.8000],\n",
      "        [0.8000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x *(1+ torch.tanh(torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x+0.044715 * torch.pow(x, 3))) )\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0000,  2.9394,  2.8788,  2.8182,  2.7576,  2.6970,  2.6364,  2.5758,\n",
       "         2.5152,  2.4545,  2.3939,  2.3333,  2.2727,  2.2121,  2.1515,  2.0909,\n",
       "         2.0303,  1.9697,  1.9091,  1.8485,  1.7879,  1.7273,  1.6667,  1.6061,\n",
       "         1.5455,  1.4848,  1.4242,  1.3636,  1.3030,  1.2424,  1.1818,  1.1212,\n",
       "         1.0606,  1.0000,  0.9394,  0.8788,  0.8182,  0.7576,  0.6970,  0.6364,\n",
       "         0.5758,  0.5152,  0.4545,  0.3939,  0.3333,  0.2727,  0.2121,  0.1515,\n",
       "         0.0909,  0.0303, -0.0303, -0.0909, -0.1515, -0.2121, -0.2727, -0.3333,\n",
       "        -0.3939, -0.4545, -0.5152, -0.5758, -0.6364, -0.6970, -0.7576, -0.8182,\n",
       "        -0.8788, -0.9394, -1.0000, -1.0606, -1.1212, -1.1818, -1.2424, -1.3030,\n",
       "        -1.3636, -1.4242, -1.4848, -1.5455, -1.6061, -1.6667, -1.7273, -1.7879,\n",
       "        -1.8485, -1.9091, -1.9697, -2.0303, -2.0909, -2.1515, -2.2121, -2.2727,\n",
       "        -2.3333, -2.3939, -2.4545, -2.5152, -2.5758, -2.6364, -2.6970, -2.7576,\n",
       "        -2.8182, -2.8788, -2.9394, -3.0000])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(3,-3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gelu , relu  = GELU() , nn.ReLU()\n",
    "x = torch.linspace(-3,3, 100)\n",
    "y_gelu , y_relu = gelu(x) , relu(x)\n",
    "plt.figure(figsize=(8 , 3))\n",
    "for i , (y  , label)  in enumerate(zip([y_gelu, y_relu] ,[\"GELU\" , \"RELU\"]) , 1):\n",
    "    plt.subplot(1,2 , i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f'{label}')\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'] , 4 * cfg['emb_dim']) ,\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg['emb_dim'] , cfg['emb_dim'])\n",
    "        )\n",
    "    def forward(self, x ):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.randn(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut connection or residual connection \n",
    "\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes , use_shortcut ):\n",
    "        super().__init__()\n",
    "        self.use_shorcut =  use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0] , layer_sizes[1]) , GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1] , layer_sizes[2]) , GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2] , layer_sizes[3]) , GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3] , layer_sizes[4]) , GELU()),\n",
    "             nn.Sequential(nn.Linear(layer_sizes[4] , layer_sizes[5]) , GELU()),\n",
    "\n",
    "        ])\n",
    "    def forward(self , x ):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shorcut and x.shape  ==  layer_output.shape:\n",
    "                x = x + layer_output\n",
    "\n",
    "            else:\n",
    "                x = layer_output    \n",
    "\n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1.,0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes=layer_sizes , use_shortcut= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model , x):\n",
    "    output = model(x)\n",
    "    target  =  torch.tensor([[0]], dtype= torch.float32)\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output ,  target)\n",
    "    loss.backward()\n",
    "    for name , parm in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name}  has gradient mean of {parm.grad.abs().mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight  has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight  has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight  has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight  has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight  has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight  has gradient mean of 0.22169798612594604\n",
      "layers.1.0.weight  has gradient mean of 0.20694111287593842\n",
      "layers.2.0.weight  has gradient mean of 0.3289700150489807\n",
      "layers.3.0.weight  has gradient mean of 0.26657330989837646\n",
      "layers.4.0.weight  has gradient mean of 1.3258544206619263\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "        def __init__(self, cfg):\n",
    "                super().__init__()\n",
    "                self.attn = MultiHeadAttention_V2(d_in=cfg['emb_dim'] , d_out=cfg['emb_dim'] ,\n",
    "                                                   context_length=cfg['context_length'] ,\n",
    "                                                     dropout=cfg['drop_rate'] , num_heads=cfg['n_heads'] ,\n",
    "                                                      qkv_bias=cfg['qkv_bias'] )\n",
    "                self.layernorm1 = LayerNorm(emb_dim=cfg['emb_dim'])\n",
    "                self.layernorm2 = LayerNorm(emb_dim=cfg['emb_dim'])\n",
    "                self.ff = FeedForward(cfg=cfg)\n",
    "                self.drop_out  =nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        def forward(self, x):\n",
    "                shortcut = x\n",
    "                x = self.layernorm1(x)\n",
    "                x = self.attn(x)\n",
    "                x = self.drop_out(x)\n",
    "                x = shortcut + x\n",
    "\n",
    "                shortcut = x \n",
    "                x = self.layernorm2(x)\n",
    "                x = self.ff(x)\n",
    "                x = self.drop_out(x)\n",
    "                x = x + shortcut\n",
    "                \n",
    "                return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, 'd_out must be divisible by num_heads'\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # Linear projections\n",
    "        queries = self.w_query(x).view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = self.w_key(x).view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = self.w_value(x).view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose for multi-head attention\n",
    "        queries = queries.transpose(1, 2)  # (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)        # (b, num_heads, num_tokens, head_dim)\n",
    "        values = values.transpose(1, 2)    # (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)  # (b, num_heads, num_tokens, num_tokens)\n",
    "        mask_bool = self.mask[:num_tokens, :num_tokens].unsqueeze(0).unsqueeze(0)\n",
    "        mask_bool = mask_bool.expand(b, self.num_heads, num_tokens, num_tokens).bool()\n",
    "        attn_scores = attn_scores.masked_fill(mask_bool, float('-inf'))\n",
    "\n",
    "        # Attention weights\n",
    "        attn_weights = torch.softmax(attn_scores / (self.head_dim ** 0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Context vector\n",
    "        context_vector = (attn_weights @ values).transpose(1, 2)  # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)  # (b, num_tokens, d_out)\n",
    "\n",
    "        # Output projection\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "        return context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "        d_in=cfg[\"emb_dim\"],\n",
    "        d_out=cfg[\"emb_dim\"],\n",
    "        context_length=cfg[\"context_length\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout=cfg[\"drop_rate\"],\n",
    "        qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "    #A\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut # Add the original input back\n",
    "        shortcut = x #B\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut #C\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1649,  0.4003, -0.0746,  ...,  1.2644,  0.3327,  0.7242],\n",
       "         [ 0.0295,  0.0499,  0.2529,  ...,  0.4699,  0.1284,  0.9746],\n",
       "         [ 0.5534,  0.5785, -0.0309,  ...,  1.1541,  0.3949,  0.7598],\n",
       "         [ 0.1631,  0.7129,  0.7272,  ...,  0.3312,  0.5731,  0.9255]],\n",
       "\n",
       "        [[ 0.1788,  1.1680,  0.5809,  ...,  0.1828,  0.0076, -0.5598],\n",
       "         [-0.2919,  0.6317,  0.2002,  ...,  0.3218,  0.4671, -0.0381],\n",
       "         [ 0.9273,  0.4202,  0.3183,  ...,  0.3771,  0.7189, -0.1204],\n",
       "         [ 0.6033,  0.5767,  0.3411,  ...,  1.3796,  1.2681,  0.3915]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1649,  0.4003, -0.0746,  ...,  1.2644,  0.3327,  0.7242],\n",
       "         [ 0.0295,  0.0499,  0.2529,  ...,  0.4699,  0.1284,  0.9746],\n",
       "         [ 0.5534,  0.5785, -0.0309,  ...,  1.1541,  0.3949,  0.7598],\n",
       "         [ 0.1631,  0.7129,  0.7272,  ...,  0.3312,  0.5731,  0.9255]],\n",
       "\n",
       "        [[ 0.1788,  1.1680,  0.5809,  ...,  0.1828,  0.0076, -0.5598],\n",
       "         [-0.2919,  0.6317,  0.2002,  ...,  0.3218,  0.4671, -0.0381],\n",
       "         [ 0.9273,  0.4202,  0.3183,  ...,  0.3771,  0.7189, -0.1204],\n",
       "         [ 0.6033,  0.5767,  0.3411,  ...,  1.3796,  1.2681,  0.3915]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'] , cfg['emb_dim'])\n",
    "\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'] , cfg['emb_dim'])\n",
    "\n",
    "        self.drop_rate = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.transformer_block  = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "\n",
    "        self.final_norm  =  LayerNorm(emb_dim=cfg['emb_dim'])\n",
    "\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'] , cfg['vocab_size'] , bias = False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "\n",
    "        batch_size , seq_len = idx.shape\n",
    "\n",
    "        token_embs = self.tok_emb(idx)\n",
    "\n",
    "        pos_embs = self.pos_emb(torch.arange(seq_len , device= idx.device))\n",
    "\n",
    "        x = token_embs + pos_embs\n",
    "\n",
    "        x  =  self.drop_rate(x)\n",
    "\n",
    "        x = self.transformer_block(x)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        x = self.out_head(x)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0079, -0.1957,  ..., -0.0222, -0.1062,  0.1717],\n",
      "         [ 0.3867, -0.8400, -0.6558,  ..., -0.5162,  0.2362, -0.3349],\n",
      "         [ 0.6985, -0.1826, -0.1634,  ...,  0.1472, -0.6503, -0.0054],\n",
      "         [-0.4288,  0.1670, -0.1262,  ...,  1.1571,  0.5297, -0.5542]],\n",
      "\n",
      "        [[ 0.1095, -0.2890, -0.1463,  ..., -0.0557,  0.2907, -0.2818],\n",
      "         [ 0.0884, -0.3545, -0.3524,  ...,  1.2921,  0.0050,  0.1902],\n",
      "         [ 0.6092,  0.4702, -0.4093,  ...,  0.7682,  0.3781, -0.1968],\n",
      "         [-0.0608, -0.0739,  0.4747,  ...,  1.2458, -0.3834,  0.0612]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in the GPT Model :163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_parms  = sum( p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters in the GPT Model :{total_parms:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_parms * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_text(model  , idx , context_size ,max_new_tokens ):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            print(logits.shape)\n",
    "        logits  = logits[:, -1  , :]\n",
    "        print(logits.shape)\n",
    "        probs  = torch.softmax(logits  , dim=-1)\n",
    "        print(probs)\n",
    "        idx_next = torch.argmax(probs, dim=-1 , keepdim= True)\n",
    "        print(idx_next)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0001,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[27018]])\n",
      "torch.Size([1, 5, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[24086]])\n",
      "torch.Size([1, 6, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[47843]])\n",
      "torch.Size([1, 7, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[30961]])\n",
      "torch.Size([1, 8, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[42348]])\n",
      "torch.Size([1, 9, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[7267]])\n",
      "torch.Size([1, 10, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[49706]])\n",
      "torch.Size([1, 11, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[43231]])\n",
      "torch.Size([1, 12, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[47062]])\n",
      "torch.Size([1, 13, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[34657]])\n",
      "torch.Size([1, 14, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[18631]])\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657, 18631]])\n",
      "Output length: 15\n"
     ]
    }
   ],
   "source": [
    "model.eval() #A\n",
    "out = Generate_text(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=11,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient Transformers (Sparse Attention):\n",
    "\n",
    "#     Use transformers like Longformer, BigBird, or Reformer that process long sequences with sparse attention mechanisms.\n",
    "#     These models focus on critical parts of the context instead of attending to all tokens.\n",
    "#     Use Case: Applications with long text but limited compute resources.\n",
    "#     Pros:\n",
    "#         Handles very long contexts efficiently.\n",
    "#     Cons:\n",
    "#         Requires specialized transformer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_idx(text,  tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_idx_to_text(tokens , tokenizer):\n",
    "    flat  = tokens.squeeze(0)\n",
    "    decode = tokenizer.decode(flat.tolist())\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "encode  =  text_to_token_idx('hello' , tokenizer)\n",
    "print(token_idx_to_text(encode, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[24086]])\n",
      "torch.Size([1, 6, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[47843]])\n",
      "torch.Size([1, 7, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[30961]])\n",
      "torch.Size([1, 8, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[42348]])\n",
      "torch.Size([1, 9, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[7267]])\n",
      "torch.Size([1, 10, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[19682]])\n",
      "torch.Size([1, 11, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[43231]])\n",
      "torch.Size([1, 12, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[47062]])\n",
      "torch.Size([1, 13, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0001]])\n",
      "tensor([[34657]])\n",
      "torch.Size([1, 14, 50257])\n",
      "torch.Size([1, 50257])\n",
      "tensor([[    0.0000,     0.0000,     0.0000,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[18631]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = Generate_text(model= model , idx=text_to_token_idx(\"Every effort moves yoy\" , tokenizer),\n",
    "                          max_new_tokens=10 ,\n",
    "                          context_size=GPT_CONFIG_124M['context_length']\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves yoyiman Byeswickattribute argueternity Normandy Compton analogous bore\n"
     ]
    }
   ],
   "source": [
    "print(token_idx_to_text(token_ids , tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combined_qa.txt' ,'r') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = text_data[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_char = len(text_data[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4920\n"
     ]
    }
   ],
   "source": [
    "total_toke  =  len(tokenizer.encode(text_data))\n",
    "print(total_toke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    }
   ],
   "source": [
    "# Training dataset \n",
    "train_ratio = 0.90\n",
    "split = int(train_ratio * len(text_data))\n",
    "print(split)\n",
    "train_data= text_data[:split]\n",
    "val_data = text_data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset , DataLoader \n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt) #A\n",
    "        for i in range(0, len(token_ids) - max_length, stride): #B\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self): #C\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx): #D\n",
    "         return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4,\n",
    "    max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") #A\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #B\n",
    "    dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader_v1(txt= train_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  True , drop_last=True , stride=GPT_CONFIG_124M['context_length'])\n",
    "val_dataloader = create_dataloader_v1(txt= val_data , batch_size= 2 , max_length=GPT_CONFIG_124M['context_length'] , shuffle =  False , drop_last=False , stride=GPT_CONFIG_124M['context_length'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(train_dataloader))[0][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the image79 ? gray\\nwhat is the rectangular shaped object on the night stand on the right side of the bed in the image79 ? laptop\\nwhat is in the corner of the wall on the left side of the bookshelf in the image80 ? hockey_stick\\nwhat color is the hat on the top right of the bookshelf in the image80 ? black, white\\nwhat color is the bookshelf in front of wall in the image80 ? brown\\nwhat is the red object on the dresser in the image82 ? deoderant\\nhow many knobs are in the first row of the drawers in the image82 ? 5\\nwhat color is the blanket on the mattress in the image82 ? brown\\nwhat is on the top of the dresser close to the red deoderant in the image82 ? box\\nhow many tables are in front of the bookshelf close to the ladder in the image84 ? 1\\nwhat are the blue and white objects above the tables on the left side of the picture in the image84 ? umbrella\\nwhat color are the bookshelves in this picture in the image84 ? brown\\nhow many ladders are in this picture in the image87 ? 1\\nwhat color are the bookshelves in the image87 ? brown\\nwhat is the most frequent object in this picture in the image87 ? book\\nwhat colors are the backpacks in the store in the image91 ? pink, purple, blue, black\\nwhat is the red object on the white column in the image91 ? fire_alarm\\nwhat is in the cartons close to the notebooks in the image93 ? file_holder\\nwhat color is the lamp shade near the laptop on the table in the image93 ? red\\nwhich object is made of glass and wood in this picture in the image93 ? display_case\\nwhat color is the wall behind the laptop and lampshade in the image93 ? purple\\nwhat colors are the sweatshirts in front of the store in the image95 ? gray, blue, black\\nhow many stairs are between the front side and the back side of the store in the image95 ? 3\\nwhat colors are the mugs in the shelf in the image97 ? white, black, purple, blue\\nwhat are the shelves made of in the image97 ? glass\\nwhat colors are the sweatshirts in the image99 ? pink, blue, black, white\\nhow many shelves does the bookcase have on the left side of the sweatshirts in the image99 ? 6\\nhow many steps does the step stool have in the image104 ? 3\\nwhat colors are the sticker notes on the shelves in the image104 ? pink, yellow\\nhow many shelves does the bookcase have close to the step stool in the image104 ? 7\\nwhat is on the top of the monitor on the cash desk in the image114 ? soft_toy\\nhow many lamps are above the cash desk in this picture in the image114 ? 2\\nwhat is under the counter and behind the floor mat in the image123 ? radiator\\nwhat is on the top of the counter between the paper cups and the paper boxes in the image123 ? napkin_dispenser\\nhow many blackboards are in the picture in the image123 ? 3\\nhow many tumblers are on the counter in the image124 ? 4\\nwhat is between the white mug and the paper cups on the counter in the image124 ? ornamental_plant\\nwhat color is the counter in the image124 ? white, brown\\nwhat is on the cabinet in front of the dish rack in the image130 ? casserole_dish\\nhow many plates are in the dish rack in the image130 ? 2\\nhow many cabinets are above the sink and the dish rack in the image130 ? 3\\nhow many tiers does the spice rack have in the image133 ? 3\\nwhat is hanging on the wall between the spice rack and the cabinet in the image133 ? kichen_towel\\nwhat color is the cabinet above the sink in the image133 ? white\\nwhat is the electrical object on the cabinet in the image134 ? blender\\nwhat color is the kitchen towel hanging on the cabinet handle in the image134 ? yellow\\nwhat is on the refridgerator door in the image134 ? picture\\nwhat is above the oven on the wall in the image136 ? microwave\\nwhat is in the rack on the wall in the image136 ? spice_bottle\\nwhat is the dish on the oven made of in the image136 ? aluminium_foil\\nwhat color is the toaster oven on the top of the cabinet in the image139 ? white\\nwhat color is the salt shaker below the electrical outlet in the image139 ? red\\nhow many electrical objects are on the cabinet in the image139 ? 2\\nwhat is on the cabinet in front of the electrical kettle in the image140 ? vegetable_peeler\\nwhat is in front of the oven rack'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "for x , y in train_dataloader:\n",
    "    print(x.shape , y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x , y in val_dataloader:\n",
    "    print(x.shape , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 319,  262, 1353,  ..., 9582, 1666,  287],\n",
       "         [ 262, 1353,  286,  ...,  290, 2330, 2134]]),\n",
       " tensor([[ 262, 1353,  826,  ..., 1666,  287,  262],\n",
       "         [1353,  286,  262,  ..., 2330, 2134,  319]])]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss_batch(input_batch , target_batch, model , device):\n",
    "    input_batch , target_batch = input_batch.to(device) , target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(   logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader , model , device , num_batches = None):\n",
    "    total_loss = 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches  = min(num_batches , len(data_loader))\n",
    "    for i , (inputs , target) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss  =  cal_loss_batch(inputs , target , model , device)\n",
    "\n",
    "            total_loss +=loss.item()\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        return total_loss  / num_batches\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #A\n",
    "# model.to(device)\n",
    "# train_loss = calc_loss_loader(train_dataloader, model, device) #B\n",
    "# val_loss = calc_loss_loader(val_dataloader, model, device)\n",
    "# print(\"Training loss:\", train_loss)\n",
    "# print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from typing import List , Tuple ,Dict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_step(model:nn.Module , loss_fn , optimizer:torch.optim.Optimizer ,\n",
    "                data_loader:torch.utils.data.DataLoader ,\n",
    "                  device:torch.device ) -> Tuple[float , float]:\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    for batch , (inputs , target ) in enumerate(data_loader):\n",
    "        inputs , targets  = inputs.to(device) , targets.to(device)\n",
    "        logits =  model(inputs)\n",
    "        loss = loss_fn(logits , targets) \n",
    "        train_loss +=loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(data_loader)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_step(model:nn.Module,loss_fn  , data_loader:torch.utils.data.DataLoader ,\n",
    "               device:torch.device)->Tuple[float , float]:\n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X , y in data_loader:\n",
    "            X  , y = X.to(device) , y.to(device)\n",
    "            predictions = model(X)\n",
    "            loss = loss_fn(predictions , y )\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(data_loader)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def train_model(model:nn.Module ,train_dataloader:torch.utils.data.DataLoader ,\n",
    "           test_dataloader:torch.utils.data.DataLoader , optimizer:torch.optim.Optimizer ,\n",
    "            loss_fn:nn.Module  , device:torch.device = None , epochs:int = 1  ):\n",
    "    results = {\n",
    "        \"train_loss\":[],\n",
    "        \"test_loss\":[]\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model= model , \n",
    "                                loss_fn=loss_fn , \n",
    "                                optimizer= optimizer , data_loader=train_dataloader ,device=device)\n",
    "        test_loss = test_step(model=model , loss_fn= loss_fn , data_loader= test_dataloader   ,device= device)\n",
    "        print(\n",
    "            f\"Epoch:{epoch}|\"\n",
    "            f\"train_loss:{train_loss}|\"\n",
    "            f\"test_loss:{test_loss}|\"\n",
    "\n",
    "        )\n",
    "        results['train_loss'].append(train_loss.item() if isinstance(train_loss , torch.Tensor)  else train_loss)\n",
    "        results['test_loss'].append(test_loss.item() if isinstance(test_loss , torch.Tensor)  else test_loss)\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model= model , train_dataloader= train_dataloader , test_dataloader= val_dataloader ,optimizer=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
